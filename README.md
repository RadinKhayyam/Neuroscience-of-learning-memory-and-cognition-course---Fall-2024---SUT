# üìä Neuroscience of Learning, Memory, and Cognition

**üèõÔ∏è University:** Sharif University of Technology  
**üè¢ Department:** Electrical Engineering  
**üë®‚Äçüè´ Instructor:** Prof. Hamid Aghajan

---

## üìò Course Overview

This course delves into the computational and mathematical modeling of neural systems with an emphasis on understanding how the brain learns, remembers, and makes decisions. It covers various models, including feedforward, recurrent, and multi-layer perceptron neural networks, with reinforcement learning, supervised, and unsupervised learning methods. Students will explore the dynamics of neuron spikes, decision-making processes, synaptic plasticity, and the entropy and information theories behind neural coding. Additionally, recent studies in neural computation and practical lab work are incorporated to provide hands-on experience in the field.

**Textbooks:**
- Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems, Peter Dayan and Larry Abbott, MIT Press, 2005.
- Neuronal Dynamics - From single neurons to networks and models of Cognition, W. Gerstner, W. Kistler, R. Naud, L. Paninski, Cambridge University Press, 2014.
- Dynamical Systems in Neuroscience: The Geometry of Excitability and Bursting, Eugene Izhikevich, MIT Press, 2010.

---

## üìù Syllabus

### Part I: Neuronal Networks

1Ô∏è‚É£ **Neuron Models**  
- Fascinating brain  
- Neurons, ion channels, spikes  
- The Hodgkin-Huxley equation  
- Anatomy of a spike  
- Integrate-and-fire neurons  
- Two-dimensional models  

2Ô∏è‚É£ **Feedforward & Recurrent Networks**  
- From spiking to firing-rate based network  
- Edge detection in the brain, oriented gradients  
- Amplification and memory  
- Nonlinear recurrent networks  
- Excitatory-inhibitory networks  
- Phase plane stability analysis  

3Ô∏è‚É£ **Synaptic Plasticity & Learning**  
- Hebb's learning rule  
- Long-term potentiation/depression  
- Clustering as unsupervised learning  
- Expectation Maximization algorithm  
- Classification as unsupervised learning  
- Multi-layer perceptron and regression  
- Back propagation in sigmoid networks  
- Sparse coding for efficient reconstruction  
- Adaptation vs. hardwiring in brain  
- Automation of tasks and specialization of brain circuits  
- Self-organizing maps  

---

### Part II: The Neural Code

4Ô∏è‚É£ **Encoding of Information**  
- Spatiotemporal receptive fields  
- Spike-triggered average  
- Nonlinearity, Bayesian model, role of priors  
- Natural stimuli, KL divergence  

5Ô∏è‚É£ **Decoding of Spike Events**  
- Neural decoding, signal detection theory  
- Population codes  
- Optimal methods, ML & MAP estimation  

6Ô∏è‚É£ **Information & Entropy**  
- Entropy and mutual information  
- Maximizing entropy for single neuron / population of neurons  
- Adaptive neural codes  

---
### Part III: Learning, Memory, Decision

7Ô∏è‚É£ **Reinforcement Learning**  
- Classical conditioning  
- Temporal difference (TD) learning  
- Prediction of reward in the brain: Dopamine  
- Actions & policy  
- Exploration vs. exploitation  
- Actor-critic learning algorithm  

8Ô∏è‚É£ **Associative Memory**  
- Hebbian learning of associations  
- Hopfield model  
- Pattern retrieval  
- Attractor networks  
- Stochastic Hopfield model  
- Generalizations to match biology  

9Ô∏è‚É£ **Decision Models / Mind & Consciousness**  
- Population activity  
- Mean-field argument: asynchronous state  
- Transients: Generalized linear model (GLM)  
- Theory of decision dynamics  
- Solutions: Symmetric case, weak & strong biased cases  
- Libet experiment: Volition & free will  
- What decides? Who decides?  
- The issue of consciousness  

---

## üéØ Prerequisites  
- Differential equations, or Circuit theory I  
- Familiarity with Python  

## üìä Grade Distribution  
- **45%** Exercises  
- **45%** Exams  
- **10%** Paper presentation (+5% Project covers final exam)  
